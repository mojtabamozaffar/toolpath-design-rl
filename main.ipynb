{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 28 22:43:20 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.88       Driver Version: 418.88       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     On   | 00000000:19:00.0 Off |                  Off |\n",
      "| 34%   56C    P2    86W / 260W |   2550MiB / 48571MiB |     19%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     On   | 00000000:1A:00.0 Off |                  Off |\n",
      "| 38%   61C    P8    35W / 260W |     11MiB / 48571MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     On   | 00000000:67:00.0 Off |                  Off |\n",
      "| 39%   63C    P2    88W / 260W |   1180MiB / 48571MiB |     17%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     On   | 00000000:68:00.0  On |                  Off |\n",
      "| 34%   50C    P8    32W / 260W |     94MiB / 48568MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     21094      C   python                                      2539MiB |\n",
      "|    2     32350      C   .../mmv664/anaconda3/envs/myenv/bin/python  1169MiB |\n",
      "|    3      1726      G   /usr/lib/xorg/Xorg                            40MiB |\n",
      "|    3      1763      G   /usr/bin/gnome-shell                          41MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from environment import create_am_env\n",
    "from shared_storage import SharedStorage\n",
    "from replay_buffer import ReplayBuffer\n",
    "from self_play import SelfPlay\n",
    "from trainer import Trainer\n",
    "\n",
    "class MuZeroConfig(object):\n",
    "    def __init__(self):\n",
    "        self.description = 'am'\n",
    "        self.observation_shape = (1, 32, 32)\n",
    "        self.action_space_size = 8\n",
    "        self.stacked_observations = 0\n",
    "        self.max_moves = 300\n",
    "        self.support_size_value = 20\n",
    "        self.support_size_reward = 1\n",
    "        self.num_simulations = 50\n",
    "        self.discount = 0.997\n",
    "        self.temperature_threshold = 60\n",
    "        self.root_dirichlet_alpha = 0.25\n",
    "        self.root_exploration_fraction = 0.25\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "        self.blocks = 2\n",
    "        self.channels = 8\n",
    "        self.reduced_channels_reward = 8\n",
    "        self.reduced_channels_value = 8\n",
    "        self.reduced_channels_policy = 8 \n",
    "        self.resnet_fc_reward_layers = []\n",
    "        self.resnet_fc_value_layers = []\n",
    "        self.resnet_fc_policy_layers = []  \n",
    "        self.value_loss_weight = 0.25\n",
    "        self.n_training_loop = 200\n",
    "        self.n_episodes = 20\n",
    "        self.n_epochs = 400\n",
    "        self.eval_episodes = 1\n",
    "        self.window_size = 1000\n",
    "        self.batch_size = 512\n",
    "        self.num_unroll_steps = 10\n",
    "        self.td_steps = 50\n",
    "        self.momentum = 0.9\n",
    "        self.lr_init = 0.005\n",
    "        self.lr_decay_rate = 1.0\n",
    "        self.lr_decay_steps = 1000\n",
    "        self.seed = 0\n",
    "        self.use_last_model_value = False\n",
    "        self.logdir='results/{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\"),self.description)\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.weight_decay = 1e-4\n",
    "        self.visit_softmax_temperature_fn = lambda x: 1.0 if x<0.5*self.n_training_loop*self.n_epochs else (\n",
    "                                                      0.5 if x<0.75*self.n_training_loop*self.n_epochs else \n",
    "                                                      0.25)\n",
    "\n",
    "config = MuZeroConfig()    \n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "\n",
    "os.makedirs(config.logdir, exist_ok=True)\n",
    "writer = SummaryWriter(config.logdir)\n",
    "\n",
    "env, _, env_test = create_am_env(max_steps = config.max_moves)\n",
    "storage = SharedStorage(config)\n",
    "replay_buffer = ReplayBuffer(config)\n",
    "trainer = Trainer(storage, replay_buffer, config)\n",
    "train_worker = SelfPlay(storage, replay_buffer, env, config)\n",
    "test_worker = SelfPlay(storage, replay_buffer, env_test, config, test_mode=True)\n",
    "\n",
    "hp_table = [\"| {} | {} |\".format(key, value) for key, value in config.__dict__.items()]\n",
    "writer.add_text(\"Hyperparameters\",\"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(hp_table))\n",
    "\n",
    "for loop in range(config.n_training_loop):\n",
    "#     start = time.time()\n",
    "    train_worker.play(loop)\n",
    "#     end = time.time()\n",
    "#     print(\"Total train worker self-play: {:.2f}\".format((end-start)/60))\n",
    "#     start = time.time()\n",
    "    test_worker.play(loop)\n",
    "#     end = time.time()\n",
    "#     print(\"Total test worker self-play: {:.2f}\".format((end-start)/60))\n",
    "#     start = time.time()\n",
    "    trainer.train()\n",
    "#     end = time.time()\n",
    "#     print(\"Total training: {:.2f}\".format((end-start)/60))\n",
    "\n",
    "    infos = storage.get_infos()\n",
    "    writer.add_scalar(\"1.Reward/1.Total reward\", infos[\"total_reward\"], loop)\n",
    "    writer.add_scalar(\"1.Reward/2.Mean value\", infos[\"mean_value\"], loop)\n",
    "    writer.add_scalar(\"2.Workers/1.Self played games\",replay_buffer.get_self_play_count(),loop)\n",
    "    writer.add_scalar(\"2.Workers/2.Training steps\", infos[\"training_step\"], loop)\n",
    "    writer.add_scalar(\"2.Workers/3.Self played games per training step ratio\", \n",
    "                      replay_buffer.get_self_play_count()/ max(1, infos[\"training_step\"]),loop)\n",
    "    writer.add_scalar(\"2.Workers/4.Learning rate\", infos[\"lr\"], loop)\n",
    "    writer.add_scalar(\"3.Loss/1.Total loss\", infos[\"total_loss\"], loop)\n",
    "    writer.add_scalar(\"3.Loss/Value loss\", infos[\"value_loss\"], loop)\n",
    "    writer.add_scalar(\"3.Loss/Reward loss\", infos[\"reward_loss\"], loop)\n",
    "    writer.add_scalar(\"3.Loss/Policy loss\", infos[\"policy_loss\"], loop)\n",
    "    print(\"[{}] Reward: {:.2f}. Training step: {}/{}. Played games: {}. Loss: {:.2f}\".format(\n",
    "                        str(datetime.datetime.now().strftime('%H:%M:%S')),\n",
    "                        infos[\"total_reward\"],\n",
    "                        loop,\n",
    "                        config.n_training_loop,\n",
    "                        replay_buffer.get_self_play_count(),\n",
    "                        infos[\"total_loss\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
